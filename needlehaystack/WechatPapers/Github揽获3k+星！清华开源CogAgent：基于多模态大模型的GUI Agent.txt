​Github揽获3k+星！清华开源CogAgent：基于多模态大模型的GUI Agent                                                洪文逸                                                  PaperWeekly              PaperWeekly微信号paperweekly功能介绍PaperWeekly是一个推荐、解读、讨论和报道人工智能前沿论文成果的学术平台，致力于让国内外优秀科研工作得到更为广泛的传播和认可。社区：http://paperweek.ly | 微博：@PaperWeekly发表于©作者 | 洪文逸单位 | 清华大学博士生研究方向 | 多模态学习论文名称：CogAgent: A Visual Language Model for GUI Agents论文链接：https://arxiv.org/pdf/2312.08914.pdfGitHub项目地址（含开源模型、网页版Demo）：https://github.com/THUDM/CogVLM我们提出了视觉 GUI Agent，使用视觉模态（而非文本）对 GUI 界面进行更全面直接的感知，从而做出规划和决策。对此，我们研发了多模态大模型 CogAgent，可接受 1120×1120 的高分辨率图像输入，不仅提升了通用视觉理解能力，还具备强大的 GUI Agent 能力。CogAgent 是一个通用的视觉理解大模型，具备视觉问答、视觉定位（Grounding）、GUI Agent 等多种能力，在 9 个经典的图像理解榜单上（含 VQAv2，STVQA, DocVQA，TextVQA，MM-VET，POPE 等）取得了通用能力第一的成绩，并在涵盖电脑、手机的GUI Agent数据集上（含 Mind2Web，AITW 等），大幅超过基于 LLM 的 Agent，取得第一。为了更好地促进多模态大模型、Agent 社区的发展，我们已将 CogAgent-18B 开源至 GitHub 仓库（可商用），并提供了网页版 Demo。欢迎大家体验、使用与反馈！视觉GUI Agent基于语言预训练模型（LLM）的 Agent 是当下热门的研究话题，具备良好的应用前景。但是，一个严重的问题是，受限于 LLM 的模态，它只能接受语言形式的输入。拿网页 agent 为例，WebAgent [3] 等工作将网页 HTML 连同用户目标（例如 “Can you search for CogAgent on google”）作为 LLM 的输入，从而获得 LLM 对下一步动作的预测（例如点击按钮，输入文本）。然而，一个有趣的观察是，人类是通过视觉与 GUI 交互的。比如，面对一个网页，当给定一个操作目标时，人类会先观察他的 GUI 界面，然后决定下一步做什么；与此同时，GUI 天然是为了人机交互设计的，相比于 HTML 等文本模态的表征，GUI 更为直接简洁，易于获取有效信息。也就是说，在 GUI 场景下，视觉是一种更为直接、本质的交互模态，能更高效完整提供环境信息；更进一步地，很多 GUI 界面并没有对应的源码，也难以用语言表示。因此，若能将大模型改进为视觉 Agent，将 GUI 界面以视觉的形式直接输入大模型中用于理解、规划和决策，将是一个更为直接有效、具备极大提升空间的方法。对此，我们提出了多模态大模型 CogAgent，可以实现基于视觉的 GUI Agent。下图展现了其工作路径与能力。CogAgent 模型同时接受当前 GUI 截图（图像形式）和用户操作目标（文本形式，例如 “search for the best paper in CVPR 2023”）作为输入，就能预测详细的动作，和对应操作元素的位置坐标。CogAgent 可以应用于包括电脑、手机的各种场景。受益于 GUI Agent 的可泛化性，CogAgent 能在各类没见过的场景与任务上都取得良好的性能。下图是手机场景下的一个例子。论文中展示了更多示例，覆盖了 PPT、手机系统、社交软件、游戏等各类场景。一个手机系统的例子一个 Google Map 的例子一个原神的例子User：Who are the teammates on the current mission? 当前任务中的队友是谁？CogAgent：The teammates on the current mission are Raiden Shogun, Bennett, and Kaedehara Kazuha. 当前任务中的队友是Raiden Shogun, Bennett, and Kaedehara Kazuha。User：I'm looking for guidance on how to "Find out how to view a detailed map"? 我想知道如何“查看详细地图”？CogAgent：Plan: 1. Identify the map icon on the screen. 2. Click on the map icon to open the detailed map view.  计划：1. 确定屏幕上的地图图标。2. 点击地图图标打开详细地图视图。Next Action: Move the cursor over the top left corner of the screen and click on the compass-like icon with 'N' inside it.  下一步操作：将光标移动到屏幕左上角，点击内有 “N” 的类似指南针的图标。模型结构及训练方法CogAgent 的模型结构基于 CogVLM [2]。为了使模型具备对高分辨率图片的理解能力，可以看清 ~720p 的 GUI 屏幕输入，我们将图像输入的分辨率大幅提升至 1120×1120（以往的模型通常小于 500×500，包括 CogVLM，Qwen-VL 等）。然而，分辨率的提升会导致图像序列急剧增长，带来难以承受的计算和显存开销——这也是现有多模态预训练模型通常采用较小分辨率图像输入的原因之一。对此，我们设计了轻量级的“高分辨率交叉注意力模块”，在原有低分辨率大图像编码器（4.4B）的基础上，增加了高分辨率的小图像编码器（0.3B），并使用交叉注意力机制与原有的 VLM 交互。在交叉注意力中，我们也使用了较小的 hidden size，从而进一步降低显存与计算开销。结果表明，该方法可以使模型成功理解高分辨率的图片，并有效降低了显存与计算开销。在消融实验中，我们比较了该结构与 CogVLM 原始方法的计算量。结果表明，当分辨率提升时，使用文中提出的方案（with cross-module，橙色）将会带来极少量的计算量增加，并与图像序列的增长成线性关系。特别的，1120×1120 分辨率的 CogAgent 的计算开销（FLOPs），甚至比 490×490 分辨率的 CogVLM 的 1/2 还要小。在 INT4 单卡推理测试中，1120×1120 分辨率的 CogAgent 模型占用约 12.6GB 的显存，相较于 224×224 分辨率的 CogVLM 仅高出不到 2GB。在数据方面，除了 CogVLM 用到的 image caption 数据集之外，我们在文本识别、视觉定位、GUI 图像理解方面进行了数据扩充与增强，从而有效提升了 GUI Agent 场景下的性能。CogAgent 的预训练和微调数据的采集、生成方法详细介绍于论文的 2.2 和 2.3 部分。CogAgent 模型的训练 batch size 为 4,608，学习率为 2e-5，共计训练了 6 万迭代步。实验结果3.1 GUI Agent能力在电脑、手机等 GUI Agent 的数据集上，CogAgent 具有较大的优势，大幅超过所有基于 LLM 的 Agent：在网页 Agent 数据集 Mind2Web 上的性能在手机 Agent 数据集 AITW 上的性能3.2 图像理解综合能力CogAgent 在图像理解的综合能力也有相当的提升，在 9 个经典的图像理解榜单上（含 VQAv2，STVQA, DocVQA，TextVQA，MM-VET，POPE 等）取得了通用能力第一的成绩。在 VQA 数据集上的通用性能，涵盖常识、OCR、图表、文档等方面：在多模态大模型通用榜单 MM-VET，POPE 上的结果：彩蛋一个小彩蛋 🎁：若提问 CogAgent，应该如何给 CogVLM 仓库点星标（作者按：这也是 CogAgent 的仓库），可以将截图和问题 Can you advise me on how to "star cogvlm repo" 输入给模型：CogAgent 会告诉你具体动作及对应元素的坐标（红框为模型预测坐标对一个的位置）：欢迎大家体验使用 CogAgent，留下星星与建议~更欢迎大家进行各类有趣的二创！参考文献[1] Hong, Wenyi, et al. "CogAgent: A Visual Language Model for GUI Agents." arXiv preprint arXiv:2312.08914 (2023).[2] Wang, Weihan, et al. "Cogvlm: Visual expert for pretrained language models." arXiv preprint arXiv:2311.03079 (2023).[3] Gur, Izzeddin, et al. "A real-world webagent with planning, long context understanding, and program synthesis." arXiv preprint arXiv:2307.12856 (2023).更多阅读#投 稿 通 道# 让你的文字被更多人看到 如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？答案就是：你不认识的人。总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是最新论文解读，也可以是学术热点剖析、科研心得或竞赛经验讲解等。我们的目的只有一个，让知识真正流动起来。📝 稿件基本要求：• 文章确系个人原创作品，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注 • 稿件建议以 markdown 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算📬 投稿通道：• 投稿邮箱：hr@paperweekly.site • 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者• 您也可以直接添加小编微信（pwbot02）快速投稿，备注：姓名-投稿△长按添加PaperWeekly小编🔍现在，在「知乎」也能找到我们了进入知乎首页搜索「PaperWeekly」点击「关注」订阅我们的专栏吧··预览时标签不可点