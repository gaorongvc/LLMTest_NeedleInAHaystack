《大型模型的参数高效微调》全面综述                          机器学习算法与自然语言处理              机器学习算法与自然语言处理微信号MLNLPer-World功能介绍关注AI前沿技术，助力AI学者进步MLNLP社区是国内外知名的机器学习与自然语言处理社区，受众覆盖国内外NLP硕博生、高校老师以及企业研究人员。
社区的愿景是促进国内外自然语言处理，机器学习学术界、产业界和广大爱好者之间的交流和进步，特别是初学者同学们的进步。
转载自 | 专知大型模型在多个应用领域代表了突破性的进步，使得各种任务都取得了显著的成就。
然而，它们前所未有的规模带来了重大的计算成本。
这些模型通常包含数十亿参数，需要大量计算资源来执行。
特别是，它们庞大的规模和计算需求在为特定的下游任务定制时，尤其是在计算能力受限的硬件平台上，提出了相当大的挑战。
参数高效微调（PEFT）提供了一个实用的解决方案，通过高效地适应各种下游任务来调整大型模型。
具体来说，PEFT指的是调整预训练大型模型的参数以将其适应于特定任务或领域的过程，同时最小化引入的额外参数数量或所需的计算资源。
这种方法在处理参数计数高的大型语言模型时尤为重要，因为从头开始微调这些模型可能在计算上代价高昂且资源密集，给支持系统平台设计带来了相当大的挑战。
在这项综述中，我们对各种PEFT算法进行了全面的研究，检查了它们的性能和计算开销。
此外，我们提供了使用不同PEFT算法开发的应用程序的概览，并讨论了用于减轻PEFT的计算成本的常见技术。
除了算法视角，我们还概述了各种现实世界的系统设计，以调查与不同PEFT算法相关的实施成本。
这项综述是希望理解PEFT算法及其系统实现的研究人员不可或缺的资源，提供了关于最近进展和实际应用的详细见解。
论文链接：https://arxiv.org/abs/2403.14608近期，大型模型（LMs）引起了广泛的公众兴趣。
它们理解上下文和细微差别的能力，使其能够熟练地处理跨多个领域的多样化任务，包括自然语言处理（NLP）、计算机视觉（CV）等。
在NLP领域，大型语言模型（LLMs）在包括文本生成[10]、[243]、翻译[239]、[61]、个性化聊天机器人[192]、[103]、[187]和摘要[212]等各种任务上取得了重大进步，展现了非凡的熟练度。
早期研究[10]表明，LLMs表现出高水平的泛化能力，能够将其获得的知识应用于原始训练中未包括的新任务。
这种能力通常被称为零样本学习。
尽管如此，为了进一步提升LLMs在新用户数据集和任务上的最佳性能，微调仍然是必不可少的。
由于其规模，微调LLMs的广泛采用策略涉及调整有限数量的LLM参数，同时保持其余部分不变。
这种技术，称为参数高效微调（PEFT），涉及选择性地调整少部分参数，同时保持其余部分不变。
此外，PEFT的应用范围不仅限于NLP领域，并迅速吸引了CV社区的兴趣，用于处理具有大量参数的视觉模型的微调，例如视觉变换器（ViT）和扩散模型，以及跨学科模型，如视觉-语言模型。
在这项综述中，我们系统地回顾和分类了PEFT算法的最新进展，以及与各种PEFT算法相关的系统实现成本。
图1展示了本综述的概览内容。
在第II部分，我们介绍了LLM和PEFT的一些基本概念，包括LLM的计算流程、PEFT的基础知识，以及常用的数据集和任务。
我们在第III部分根据它们的计算流程将所有类型的PEFT算法进行了分类。
在第III-A部分，我们介绍了添加型算法，这类算法要么引入额外的权重参数，要么修改激活函数。
对于那些仅需使用现有参数进行微调的算法，它们被归类为选择性方法，其介绍可以在第III-B部分找到。
在第III-C部分，我们探讨了重参数化PEFT，它构造了原始模型参数的（低维）重参数化以进行训练，同时变换权重以保持推理速度。
此外，还存在结合上述技术的算法，我们将这些分类为混合方法，在第III-D部分对它们进行了阐述。
我们还在第IV部分调查了进一步降低不同PEFT算法计算复杂度的策略，包括KV缓存管理、剪枝、量化和内存优化。
在第V部分，我们将这项综述的范围扩展到计算视角之外，涉及各种潜在的应用场景。
我们探索了将PEFT技术应用于不同模型架构的创新，包括LLMs（第V-A部分）、视觉变换器（第V-B部分）、视觉-语言对齐模型（第V-C部分）和扩散模型（第V-D部分），用于各种下游任务，强调了PEFT在多种场景中的多功能性和适用性。
在第VI部分，我们探讨了PEFT方法的系统设计挑战。
讨论包括三种先进的系统解决方案，用于PEFT的实际部署：分布式调整（第VI-B部分）、PEFT查询服务（第VI-C部分）和并发PEFT调整（第VI-D部分）。
在最后的第VII部分，我们总结了我们的综述，并从算法和系统视角提出了几个潜在的未来方向，希望为该领域的进一步研究和发展提供有价值的见解。
参数高效微调（PEFT）概览为了提升LLM在未见用户数据集和任务上的性能，微调仍然是必不可少的。
随着模型大小的增长（例如，从GPT-2的1.5B增长到GPT-3的175B），标准的全微调范式需要成千上万的GPU并行工作，这在高度效率和可持续性方面是极其低效的。
一种名为参数高效微调（PEFT）的算法应运而生，旨在通过调整最少的参数在下游任务上实现比全面微调更好的性能。
在视觉和多模态领域，大规模预训练模型的并行发展同样展示了它们有效的表示学习能力，使得从大数据集到小数据集或跨不同数据模态的适应通过微调变得可行。
因此，这种能力使得PEFT越来越吸引更广泛的研究社区。
我们根据它们的操作将PEFT算法分类为加法、选择性、重参数化和混合微调。
如图3所示，通常使用三种主要的加法微调算法：（1）适配器；（2）软提示；（3）其他。
它们在不同的额外可调模块或参数方面有所不同。
另一方面，选择性微调不需要任何额外的参数，它从骨干模型中选择一小部分参数，并仅使它们在下游任务的微调过程中可调，同时保持大多数参数不受影响。
我们基于所选参数的分组将选择性微调进行了分类：（1）非结构化遮罩；（2）结构化遮罩。
重参数化代表在两种等效形式之间转换模型参数。
具体而言，重参数化微调在训练期间引入额外的低秩可训练参数，然后这些参数在推理时与原始模型集成。
这种方法被归类为两种主要策略：（1）低秩分解，和（2）LoRA衍生物。
混合微调探索不同PEFT方法的设计空间，并结合它们的优点。
PEFT策略可以广泛地分为四个类别：加法PEFT（第III-A节），通过注入新的可训练模块或参数来修改模型架构；选择性PEFT（第III-B节），在微调期间使参数子集成为可训练的；重参数化PEFT（第III-C节），为训练构造原始模型参数的（低维）重参数化，然后等效地将其转换回推理；以及混合PEFT（第III-D节），结合不同PEFT方法的优点来构建统一的PEFT模型。
不同类型PEFT算法的概览在图4中展示。
标准的全面微调需要大量的计算开销，并且还可能损害模型的泛化能力。
为了缓解这个问题，一个广泛采用的方法是保持预训练的骨干网络不变，并且仅在模型架构中的战略位置引入最小数量的可训练参数。
在针对特定下游任务进行微调时，只有这些额外模块或参数的权重被更新，这导致存储、内存和计算资源需求的大幅减少。
由于它们的特点是增加参数，这些技术可以被称为加法调整，如图4（a）所示。
接下来，我们将讨论几种流行的加法PEFT算法。
与通过增加更多参数提高模型复杂性的加法PEFT不同，选择性PEFT微调现有参数的一个子集，以提升模型在下游任务上的性能，如图4（b）所示。
重参数化代表通过转换其参数，等效地将模型架构从一种转换到另一种。
在PEFT的背景下，这通常意味着构造一个低秩参数化以在训练期间实现参数效率的目标。
对于推理，可以将模型转换为其原始的权重参数化，确保推理速度不变。
这一过程在图4（c）中展示。
技术交流群邀请函△长按添加小助手扫描二维码添加小助手微信请备注：姓名-学校/公司-研究方向（如：小张-哈工大-对话系统）即可申请加入自然语言处理/Pytorch等技术交流群关于我们MLNLP 社区是由国内外机器学习与自然语言处理学者联合构建的民间学术社区，目前已经发展为国内外知名的机器学习与自然语言处理社区，旨在促进机器学习，自然语言处理学术界、产业界和广大爱好者之间的进步。
社区可以为相关从业者的深造、就业及研究等方面提供开放交流平台。
欢迎大家关注和加入我们。
预览时标签不可点