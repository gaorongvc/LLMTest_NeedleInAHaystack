CNN、Transformer、Uniformer之外，我们终于有了更高效的视频理解技术                          机器之心              机器之心微信号almosthuman2014功能介绍专业的人工智能媒体和产业服务平台机器之心报道编辑：Rome Rome视频理解因大量时空冗余和复杂时空依赖，同时克服两个问题难度巨大，CNN 和 Transformer 及 Uniformer 都难以胜任，Mamba 是个好思路，让我们看看本文是如何创造视频理解的 VideoMamba。
视频理解的核心目标在于对时空表示的把握，这存在两个巨大挑战：短视频片段存在大量时空冗余和复杂的时空依赖关系。
尽管曾经占主导地位的三维卷积神经网络 (CNN) 和视频 Transformer 通过利用局部卷积或长距离注意力有效地应对其中之一的挑战，但它们在同时解决这两个挑战方面存在不足。
UniFormer 试图整合这两种方法的优势，但它在建模长视频方面存在困难。
S4、RWKV 和 RetNet 等低成本方案在自然语言处理领域的出现，为视觉模型开辟了新的途径。
Mamba 凭借其选择性状态空间模型 (SSM) 脱颖而出，实现了在保持线性复杂性的同时促进长期动态建模的平衡。
这种创新推动了它在视觉任务中的应用，正如 Vision Mamba 和 VMamba 所证实的那样，它们利用多方向 SSM 来增强二维图像处理。
这些模型在性能上与基于注意力的架构相媲美，同时显著减少了内存使用量。
鉴于视频产生的序列本身更长，一个自然的问题是：Mamba 能否很好地用于视频理解？受 Mamba 启发，本文引入了 VideoMamba 专为视频理解量身定制的纯 SSM (选择性状态空间模型)。
VideoMamba 以 Vanilla ViT 的风格，将卷积和注意力的优势融合在一起。
它提供一种线性复杂度的方法，用于动态时空背景建模，非常适合高分辨率的长视频。
相关评估聚焦于 VideoMamba 的四个关键能力：在视觉领域的可扩展性：本文对 VideoMamba 的可扩展性进行了检验，发现纯 Mamba 模型在不断扩展时往往容易过拟合，本文引入一种简单而有效的自蒸馏策略，使得随着模型和输入尺寸的增加，VideoMamba 能够在不需要大规模数据集预训练的情况下实现显著的性能增强。
对短期动作识别的敏感性：本文的分析扩展到评估 VideoMamba 准确区分短期动作的能力，特别是那些具有细微动作差异的动作，如打开和关闭。
研究结果显示，VideoMamba 在现有基于注意力的模型上表现出了优异的性能。
更重要的是，它还适用于掩码建模，进一步增强了其时间敏感性。
在长视频理解方面的优越性：本文评估了 VideoMamba 在解释长视频方面的能力。
通过端到端训练，它展示了与传统基于特征的方法相比的显著优势。
值得注意的是，VideoMamba 在 64 帧视频中的运行速度比 TimeSformer 快 6 倍，并且对 GPU 内存需求减少了 40 倍 (如图 1 所示)。
与其他模态的兼容性：最后，本文评估了 VideoMamba 与其他模态的适应性。
在视频文本检索中的结果显示，与 ViT 相比，其性能得到了改善，特别是在具有复杂情景的长视频中。
这凸显了其鲁棒性和多模态整合能力。
本文的深入实验揭示了 VideoMamba 在理解短期 (K400 和 SthSthV2) 和长期 (Breakfast，COIN 和 LVU) 视频内容方面的巨大潜力。
鉴于其高效性和有效性，VideoMamba 注定将成为长视频理解领域的重要基石。
所有代码和模型均已开源，以促进未来的研究努力。
论文地址：https://arxiv.org/pdf/2403.06977.pdf项目地址：https://github.com/OpenGVLab/VideoMamba论文标题：VideoMamba: State Space Model for Efficient Video Understanding方法介绍下图 2a 显示了 Mamba 模块的细节。
图 3 说明了 VideoMamba 的整体框架。
本文首先使用 3D 卷积 (即 1×16×16) 将输入视频 Xv ∈ R 3×T ×H×W 投影到 L 个非重叠的时空补丁 Xp ∈ R L×C，其中 L=t×h×w (t=T,h= H 16, 和 w= W 16)。
输入到接下来的 VideoMamba 编码器的 token 序列是时空扫描：为了将 B-Mamba 层应用于时空输入，本文图 4 中将原始的 2D 扫描扩展为不同的双向 3D 扫描：(a) 空间优先，通过位置组织空间 token，然后逐帧堆叠它们；(b) 时间优先，根据帧排列时间 token，然后沿空间维度堆叠；(c) 时空混合，既有空间优先又有时间优先，其中 v1 执行其中的一半，v2 执行全部 (2 倍计算量)。
图 7a 中的实验表明，空间优先的双向扫描是最有效但最简单的。
由于 Mamba 的线性复杂度，本文的 VideoMamba 能够高效地处理高分辨率的长视频。
对于 B-Mamba 层中的 SSM，本文采用与 Mamba 相同的默认超参数设置，将状态维度和扩展比例分别设置为 16 和 2。
参照 ViT 的做法，本文调整了深度和嵌入维度，以创建与表 1 中相当大小的模型，包括 VideoMamba-Ti，VideoMamba-S 和 VideoMamba-M。
然而实验中观察到较大的 VideoMamba 在实验中往往容易过拟合，导致像图 6a 所示的次优性能。
这种过拟合问题不仅存在于本文提出的模型中，也存在于 VMamba 中，其中 VMamba-B 的最佳性能是在总训练周期的四分之三时达到的。
为了对抗较大 Mamba 模型的过拟合问题，本文引入了一种有效的自蒸馏策略，该策略使用较小且训练良好的模型作为「教师」，来引导较大的「学生」模型的训练。
如图 6a 所示的结果表明，这种策略导致了预期的更好的收敛性。
关于掩码策略，本文提出了不同的行掩码技术，如图 5 所示，专门针对 B-Mamba 块对连续 token 的偏好。
实验表 2 展示了在 ImageNet-1K 数据集上的结果。
值得注意的是，VideoMamba-M 在性能上显著优于其他各向同性架构，与 ConvNeXt-B 相比提高了 + 0.8%，与 DeiT-B 相比提高了 + 2.0%，同时使用的参数更少。
VideoMamba-M 在针对增强性能采用分层特征的非各向同性主干结构中也表现出色。
鉴于 Mamba 在处理长序列方面的效率，本文通过增加分辨率进一步提高了性能，仅使用 74M 参数就实现了 84.0% 的 top-1 准确率。
表 3 和表 4 列出了短期视频数据集上的结果。
(a) 监督学习：与纯注意力方法相比，基于 SSM 的 VideoMamba-M 获得了明显的优势，在与场景相关的 K400 和与时间相关的 Sth-SthV2 数据集上分别比 ViViT-L 高出 + 2.0% 和 + 3.0%。
这种改进伴随着显著降低的计算需求和更少的预训练数据。
VideoMamba-M 的结果与 SOTA UniFormer 不相上下，后者在非各向同性结构中巧妙地将卷积与注意力进行了整合。
(b) 自监督学习：在掩码预训练下，VideoMamba 的性能超越了以其精细动作技能而闻名的 VideoMAE。
这一成就突显了本文基于纯 SSM 的模型在高效有效地理解短期视频方面的潜力，强调了它适用于监督学习和自监督学习范式的特点。
如图 1 所示，VideoMamba 的线性复杂度使其非常适合用于与长时长视频的端到端训练。
表 6 和表 7 中的比较突显了 VideoMamba 在这些任务中相对于传统基于特征的方法的简单性和有效性。
它带来了显著的性能提升，即使在模型尺寸较小的情况下也能实现 SOTA 结果。
VideoMamba-Ti 相对于使用 Swin-B 特征的 ViS4mer 表现出了显著的 + 6.1% 的增长，并且相对于 Turbo 的多模态对齐方法也有 + 3.0% 的提升。
值得注意的是，结果强调了针对长期任务的规模化模型和帧数的积极影响。
在 LVU 提出的多样化且具有挑战性的九项任务中，本文采用端到端方式对 VideoMamba-Ti 进行微调，取得了与当前 SOTA 方法相当或优秀的结果。
这些成果不仅突显了 VideoMamba 的有效性，也展示了它在未来长视频理解方面的巨大潜力。
如表 8 所示，在相同的预训练语料库和类似的训练策略下，VideoMamba 在零样本视频检索性能上优于基于 ViT 的 UMT。
这突显了 Mamba 在处理多模态视频任务中与 ViT 相比具有可比较的效率和可扩展性。
值得注意的是，对于具有更长视频长度 (例如 ANet 和 DiDeMo) 和更复杂场景 (例如 LSMDC) 的数据集，VideoMamba 表现出了显著的改进。
这表明了 Mamba 在具有挑战性的多模态环境中，甚至在需求跨模态对齐的情况下的能力。
更多研究细节，可参考原论文。
© THE END 转载请联系本公众号获得授权投稿或寻求报道：content@jiqizhixin.com预览时标签不可点