​ICCV 2023 | 个性化联邦学习中共性和个性特征信息的同步学习和隔离          原创                                      张剑清                                                  PaperWeekly              PaperWeekly微信号paperweekly功能介绍PaperWeekly是一个推荐、解读、讨论和报道人工智能前沿论文成果的学术平台，致力于让国内外优秀科研工作得到更为广泛的传播和认可。社区：http://paperweek.ly | 微博：@PaperWeekly©PaperWeekly 原创 · 作者 | 张剑清单位 | 上海交通大学本文介绍的是我们的一篇收录于 ICCV 2023 的论文。我们关注的是联邦学习（FL）中的模型的特征提取能力。我们提出在本地学习过程中，在特征空间引入共性信息的方法 GPFL，同步提取共性和个性信息，丰富模型中特征提取器的学习内容，同时保证了共性和个性特征信息的隔离。我们对 GPFL 就有效性、公平性、稳定性、隐私保护能力、多客户机（500个）承受能力等多个方面进行了评估。在图像、文本、传感器信号等模态的多种数据异质性场景（标签偏移、特征倾斜、真实场景等）下的实验结果表明，我们的 GPFL 不但能超越 SOTA 方法 8.99% 之多，还能缓解个性化联邦学习（pFL）在本地学习过程中的模型过拟合问题。论文标题：GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning论文链接：https://arxiv.org/pdf/2308.10279v3.pdf代码链接：https://github.com/TsingZ0/GPFL（含有Poster）https://github.com/TsingZ0/PFLlib背景和动机近年来，联邦学习（FL）因其在协作学习和隐私保护方面的能力备受关注。然而，由于在联邦学习中我们不能对源自客户机的私有数据进行操作，而每个客户机都由于自身所处环境和数据采集能力差异，使得不同客户机上的本地私有数据分布不同。这种数据异质性（如图 1 中不同的颜色所示）是影响联邦学习算法训练模型效果的最大的问题之一。目前，个性化联邦学习（pFL）这一类方法的诞生，有效地缓解了数据异质性的问题。相比于一般的联邦学习方法，个性化联邦学习方法结合了“联合”和“个性化”两大特性，在“求同”的过程中也“存异”。个性化联邦学习旨在通过协作学习促进客户机之间的信息交换，缓解本地数据不足的问题，以提升本地模型的能力。▲ 图1：个性化联邦学习及其数据异质性问题一个理想的个性化联邦学习方法，应在求同存异上达平衡，使“求同”促进“存异”，使“存异”促进“求同”。换句话说，个性化联邦学习需要同时实现两个目标：1）为协作学习部分提供尽可能纯净的共性信息以供聚合；2）为个性化部分训练尽可能符合本地任务的强大模型。然而，这两个目标其实涉及了“共性”和“个性”的冲突，这也是人类社会中长期难以平衡的两大问题。现有的个性化联邦学习方法，在本地训练过程中，往往偏向于任意一侧，难以实现共性和个性的平衡。在数据异质性场景下，本地训练过程中的个性化偏向，使得上传到服务器的模型参数中的共性信息不纯净，不利于目标（1），如下图所示。所以我们从“共性和个性平衡”的角度出发，提出了一个在本地训练过程中能同时实现这两个目标的方法 GPFL。▲ 图2：个性化联邦学习本地训练过程中的问题GPFL方法我们主要关注本地训练过程和本地模型的特征提取能力，所以这里忽略了服务器端的聚合操作和通讯等细节，仅展示了本地模型中的数据流和本地训练目标函数。我们的主要思路是：既然本地模型训练在不修改本地任务的目标函数的情况下，天然地关注更多个性化的信息，那么我们只需要在本地特征提取器训练过程中引入更多的全局特征信息，并处理好引入方式，便可同时实现上面提到的两个目标。于是，在“条件计算方法”的启发下，我提出了如图 3 所示的本地训练过程。这里为了理解方便，我们不提及公式，具体细节和公式等请参考我们的论文。▲ 图3：GPFL中的本地模型和本地训练细节首先，我们把原本的本地模型切分为两部分，一部分是特征提取器（），另一部分是一个任务相关的头部（）。我们希望特征提取器输出的特征向量  能同时吸收共性特征信息和本地任务中的个性信息。于是，我们提出了一个条件计算阀 CoV，插入到  和  之间。CoV 就像一个阀门，当输入个性信号  时，“个性信息提取路径”开启，数据流从  流到 ；当输入共性信息  时，“共性信息提取路径”开启，数据流从  流到 。这样就可以隔离掉  中的个性信息学习和  中的共性信息学习。当我们同时开启两条路径，就可以同时实现共性和个性信息的提取，且依旧保证  与  中的信息尽可能不受影响。为了保证  能尽可能提取个性信息，我们将  保留在本地不作全局聚合。通过本地训练目标 ，个性特征信息可以天然地被本地化的  提取到，但共性特征信息依旧难以直接获得。于是我们提出了可学习的 GCE 模块，在模型之外，专门用来学习和存储共性特征信息。由于我们关注的是特征向量，且 GCE需要具备更新、存储、查询（图 3 中的 look up）功能，所以我们就参考文本处理领域的 embedding 技术来构建 GCE。为了促进共性信息的生成，我们将 GCE 中存储的向量上传到服务器进行聚合。随后，我们用下载的全局向量引导  的共性信息提取。为了减少本地训练对共性信息提取的影响，我们将 GCE 拷贝一份后冻结（得到点划线框里的 GCE），然后用该冻结的 GCE 通过“幅度级别”的 （magnitude-level guidance loss）来引导 。但若本地训练不更新 GCE，GCE 将学不到有效信息，更不用说共性信息了。为了使 GCE 的更新尽可能少受本地数据影响，我们利用“角度级别”的对比学习方法构造了 （angle-level guidance loss）。在  和  的双重引导下，促使  充分地提取到共性信息。实验我们首先可视化了通过我们的 GPFL 方法学到的特征向量分布与其他相关方法的不同，如下图所示。从中可以看到，FedPer [1] 因为在本地训练过程中只关注了本地任务，提取到的特征较为杂乱。FedProto [2] 虽然也在特征空间中加入额外的共性特征信息，但它并没有将共性和个性特征信息的学习进行隔离，使得它们这两类本就冲突的信息互相干扰对方的学习，最终导致了类别 0 和 1 的簇互相粘连。而我们的 GPFL 既同时考虑了共性个性信息的同步提取，又做到了对这两类信息的隔离，提升它们的纯净度，最终使提取到的特征向量同时具备共性和个性信息。比如，图中类别 0 和 1 总共分为了 4 个簇，既在类别（共性）上做了分类，也在客户机 ID（个性）上做了分类。▲ 图4：特征向量的分布图由于引入了更多的共性信息，我们 GPFL 方法也缓解了大多数个性化联邦学习算法具有的问题：过拟合问题。如图 5 所示，在训练 loss 都已经不变，显示已经收敛之后，大多数个性化联邦学习方法的测试准确率在到达最高点后，出现了下降的现象，这就是过拟合问题。而我们的 GPFL 方法在达到最高的测试准确率后，可以稳定地保持最高准确率不变。▲ 图5：个性化联邦学习中的过拟合问题此外，我们还对比了我们 GPFL 方法在公平性方面的表现。在社会中，要实现公平，关键就在于找寻共性和个性的平衡。GPFL 在这方面也表现出色，如下表所示。▲ 表1：公平性对比当然，我们还在多种场景（图像、文本、传感器信号等）和多种数据分布模式（标签偏移、特征倾斜、真实场景等）下进行了对比实验，具体请阅读我们的论文。以下是在图像和文本任务上的两种数据异质性场景下的实验结果。▲ 表2：图像和文本任务上的对比实验（部分）参考文献[1] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated Learning with Personalization Layers. arXiv preprint arXiv:1912.00818, 2019.[2] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated Prototype Learning across Heterogeneous Clients. In AAAI, 2022.更多阅读#投 稿 通 道# 让你的文字被更多人看到 如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？答案就是：你不认识的人。总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是最新论文解读，也可以是学术热点剖析、科研心得或竞赛经验讲解等。我们的目的只有一个，让知识真正流动起来。📝 稿件基本要求：• 文章确系个人原创作品，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注 • 稿件建议以 markdown 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供业内具有竞争力稿酬，具体依据文章阅读量和文章质量阶梯制结算📬 投稿通道：• 投稿邮箱：hr@paperweekly.site • 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者• 您也可以直接添加小编微信（pwbot02）快速投稿，备注：姓名-投稿△长按添加PaperWeekly小编🔍现在，在「知乎」也能找到我们了进入知乎首页搜索「PaperWeekly」点击「关注」订阅我们的专栏吧···预览时标签不可点